Transformation and Actions:
    A Transformation is a function that produces new RDD from the existing RDDs
    but when we want to work with the actual dataset, at that point Action is performed.
    Actions give us normal java objects.

    We are building execution plan by  using transformation and when we execute action plan gets executed.
    All Transformations are Lazy.

The DAG(Execution Plan)/WebUI
    Narrow Transformation = transformation which does not involve shuffling of data over the worker nodes.
    Wide Transformation = transformation which does involve shuffling of data over the worker nodes.
                          this transformations are costly.

    partition = Chunk of data. partitions depend on the size of text file.
                if the file format is other than the text file then hash is calculated for the data and then by using module operation it
                partition is form using the data.

    Task: Chunk of code executing against the partition is called tasks.

    Shuffles:
        try to shuffle minimum data.

    Stages:
        series of transformation without any shuffle
        if spark needs shuffle then spark creates new stage
        Name of the stage in UI is name of last transformation
        Each stage needs to read the data from prev stage (Push Pull model)
        if one stage is not dependent on the output of another stage then it runs in parallel

    Salting:(adding salt value to the key)
        used to store data across the partition after the wide transformation to avoid key skewing.
        it will not allow nodes to stay idle.

    groupByKey();
        try to avoid using groupByKey() transformation as its a wide transformation and it end up shuffling most(1/3 of the data
        on the single node even if we are having more nodes and it can result into out of memory exception.


    reduceByKey()
        alternative to groupByKey()
        it has 2 operation
            1. first operation will not do any shuffling it will the just apply the function to each partition(map side reduce)
            2. second operation involves shuffling but it required very less shuffling due to partitions are reduced in the first operation

Caching and Persistence:
    cached():
        when we call action in the spark it discards the result RDD we got from last transformation
        so when we try to call any thing on that rdd the spark creates the new stage which reads the data from previous stage(that's written on disk)
        this will increase the execution time and inefficient
        1. that's why we will cache the end result for some time to avoid the extra operations (stages) using cached() function then program runs from cached() result
            but if the size of RDD is big then it will not be cached due to less memory in RAM
        2. persist(Storage Level): it will write the memory depending on given storage level(MEMORY,DISK)


